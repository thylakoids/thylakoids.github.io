<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Why do we need batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize th">
<meta property="og:type" content="article">
<meta property="og:title" content="BatchNormalization">
<meta property="og:url" content="http://example.com/2019/04/02/BatchNormalization/index.html">
<meta property="og:site_name" content="Dracarys">
<meta property="og:description" content="Why do we need batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize th">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-04-02T06:11:23.000Z">
<meta property="article:modified_time" content="2023-01-30T14:31:45.169Z">
<meta property="article:author" content="thylakoids">
<meta property="article:tag" content="deep learning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2019/04/02/BatchNormalization/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2019/04/02/BatchNormalization/","path":"2019/04/02/BatchNormalization/","title":"BatchNormalization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>BatchNormalization | Dracarys</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Dracarys</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-do-we-need-batch-normalization"><span class="nav-number">1.</span> <span class="nav-text">Why do we need batch normalization?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-dose-batch-normalization-work"><span class="nav-number">2.</span> <span class="nav-text">How dose batch normalization work?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization-in-Pytorch"><span class="nav-number">3.</span> <span class="nav-text">Batch Normalization in Pytorch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">thylakoids</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/02/BatchNormalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="thylakoids">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dracarys">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="BatchNormalization | Dracarys">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BatchNormalization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-04-02 14:11:23" itemprop="dateCreated datePublished" datetime="2019-04-02T14:11:23+08:00">2019-04-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-30 22:31:45" itemprop="dateModified" datetime="2023-01-30T22:31:45+08:00">2023-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="Why-do-we-need-batch-normalization"><a href="#Why-do-we-need-batch-normalization" class="headerlink" title="Why do we need batch normalization?"></a>Why do we need batch normalization?</h2><p>We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.</p>
<p>Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift). To explain covariance shift, let’s have a deep network on cat detection. We train our data on only black cats’ images. So, if we now try to apply this network to data with colored cats, it is obvious; we’re not going to do well. The training set and the prediction set are both cats’ images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. ( Deeplearning.ai: Why Does Batch Norm Work?<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nUUqwaxLnWs">C2W3L06</a></p>
<p>Also, batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.</p>
<h2 id="How-dose-batch-normalization-work"><a href="#How-dose-batch-normalization-work" class="headerlink" title="How dose batch normalization work?"></a>How dose batch normalization work?</h2><p>Batch normalization layer is usually used <strong>before</strong> the activation layer.</p>
<p>The basic idea is doing the normalization then applying a linear and shif to the mini-batch:</p>
<p>For input mini-batch $B = {x_{1, …, m}}$, we want to learn the parameter $\gamma$ and $\beta$. The output of the layer is ${y_i = BN_{\gamma, \beta}(x_i)}$, where:<br>$$ \mu_B \leftarrow \frac{1}{m}\sum_{i = 1}^{m}x_i $$<br>$$ \sigma_B^2 \leftarrow \frac{1}{m}\sum_{i=1}^m(x_i - \mu_B)^2 $$<br>$$ \hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $$<br>$$ y_i \leftarrow \gamma \hat{x_i} + \beta \equiv BN_{\gamma,\beta}(x_i) $$</p>
<p><strong>Usual batchnorm</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t is the incoming tensor of shape [B, H, W, C]</span></span><br><span class="line"><span class="comment"># mean and stddev are computed along 0 axis and have shape [H, W, C]</span></span><br><span class="line">mean = mean(t, axis=<span class="number">0</span>)</span><br><span class="line">stddev = stddev(t, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0.</span>.B-<span class="number">1</span>:</span><br><span class="line">  out[i,:,:,:] = norm(t[i,:,:,:], mean, stddev)</span><br></pre></td></tr></table></figure>
<p>Ba/sically, it computes <code>H*W*C</code> means and <code>H*W*C</code> standard deviations across B<br>elements. You may notice that different elements at different spatial locations<br>have their own mean and variance and gather only <code>B</code> values.</p>
<p><strong>Batchnorm in conv layer</strong></p>
<p>This way is totally possible. But the convolutional layer has a special property: filter weights are shared across the input image (you can read it in detail in this <a target="_blank" rel="noopener" href="http://cs231n.github.io/convolutional-networks/#conv">post</a>. That’s why it’s reasonable to normalize the output in the same way, so that each output value takes the mean and variance of B<em>H</em>W values, at different locations.</p>
<p>Here’s how the code looks like in this case (again pseudo-code):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t is still the incoming tensor of shape [B, H, W, C]</span></span><br><span class="line"><span class="comment"># but mean and stddev are computed along (0, 1, 2) axes and have just [C] shape</span></span><br><span class="line">mean = mean(t, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">stddev = stddev(t, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0.</span>.B-<span class="number">1</span>, x <span class="keyword">in</span> <span class="number">0.</span>.H-<span class="number">1</span>, y <span class="keyword">in</span> <span class="number">0.</span>.W-<span class="number">1</span>:</span><br><span class="line">  out[i,x,y,:] = norm(t[i,x,y,:], mean, stddev)</span><br></pre></td></tr></table></figure>
<p>In total, there are only <code>C</code> means and standard deviations and each one of them is computed over <code>B*H*W</code> values. That’s what they mean when they say “effective mini-batch”: the difference between the two is only in axis selection (or equivalently “mini-batch selection”).</p>
<h2 id="Batch-Normalization-in-Pytorch"><a href="#Batch-Normalization-in-Pytorch" class="headerlink" title="Batch Normalization in Pytorch"></a>Batch Normalization in Pytorch</h2><p> <a target="_blank" rel="noopener" href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">This link</a> provides a nice explanation of how to derive backpropagation gradients for batch normalization. Let’s see it in action now:</p>
<p> <strong>Batch Normalization for 2D data</strong></p>
<p> Let’s start with 1D normalization.<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure><br>Let’s define X and calculate its mean and variance. Note that X has size (20,100) which means it has 20 samples with each sample having 100 features (or dimensions). For normalizing, we need to look across all samples. In other words, we normalize across 20 values for each dimension (with each value coming on a sample).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">20</span>,<span class="number">100</span>) * <span class="number">5</span> + <span class="number">10</span></span><br><span class="line">X = Variable(X)</span><br><span class="line"></span><br><span class="line">mu = torch.mean(X[:,<span class="number">1</span>])</span><br><span class="line">var_ = torch.var(X[:,<span class="number">1</span>], unbiased=<span class="literal">False</span>)</span><br><span class="line">sigma = torch.sqrt(var_ + <span class="number">1e-5</span>)</span><br><span class="line">x = (X[:,<span class="number">1</span>] - mu)/sigma</span><br></pre></td></tr></table></figure>
<p>Note that in the line above we set unbiased = False while calculating var_. This is to prevent Bessel’s correction. In other words, we want to divide by N and not N-1 while calculating the variance. x is the same as the result of batch normalization. Also note that in the code below we set affine = False. This is to prevent creation of parameters γ(k), β(k) which may scale and shift normalized data again. While training, affine is set to True.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">B = nn.BatchNorm1d(<span class="number">100</span>, affine=<span class="literal">False</span>)</span><br><span class="line">y = B(X)</span><br><span class="line"><span class="built_in">print</span>(x.data / y[:,<span class="number">1</span>].data)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.0000</span><br><span class="line">1.0000</span><br><span class="line">1.0000</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>This also works for 3D input.</p>
<p><strong>Batch Normalization for 3D inputs</strong></p>
<p>Let’s define some 3D data with 4 and variance 4:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x3 = torch.randn(<span class="number">150</span>, <span class="number">20</span>, <span class="number">100</span>) * <span class="number">2</span> + <span class="number">4</span></span><br><span class="line">x3 = Variable(x3)</span><br><span class="line">B2 = nn.BatchNorm1d(<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<p>Note that here we did not set <code>affine = False</code>. Instead, we can manually set<br>those values to what we want. To preserve normalization, we want    γ(k) = 1 and<br>β(k) = 0. These values are stored in parameters weight and bias of the<br>BatchNorm variable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">B2.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">B2.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">Y = B2(X3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Manual calculation</span></span><br><span class="line">mu = X3[:,<span class="number">0</span>,:].mean()</span><br><span class="line">sigma = torch.sqrt(torch.var(X3[:,<span class="number">0</span>,:], unbiased=<span class="literal">False</span>) + <span class="number">1e-5</span>)</span><br><span class="line">X_normalized = (X3[:,<span class="number">0</span>,:] - mu)/sigma</span><br></pre></td></tr></table></figure>
<p>In the above example, X_normalized has the same values as Y[:,0,:].</p>
<p><strong>Batch Normalization for images(or any 4D input)</strong></p>
<p>A batch of RGB images has four dimensions: (B,C,X,Y) or (B,X,Y,C) where B is batch number, C is channel number, and X, Y are locations.</p>
<p>Batch normalization in images is done along the channel axis.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">5</span>,<span class="number">25</span>,<span class="number">100</span>,<span class="number">100</span>) * <span class="number">2</span> + <span class="number">4</span></span><br><span class="line">X = Variable(X)</span><br><span class="line">B = nn.BatchNorm2d(<span class="number">25</span>, affine=<span class="literal">False</span>)</span><br><span class="line">Y = B(X)</span><br></pre></td></tr></table></figure>
<p>Here <code>Y[:,i,:,:]</code> is the same as <code>((X[:,i,:,:] - X[:,i,:,:].data.mean())/((X[:,i,:,:].data.var(unbiased=False) + 1e-5)**0.5))</code> for all valid values of.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c">Batch normalization in Neural Networks</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network">Batch Normalization in Convolutional Neural Network</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/02/28/LaTeX_in_markdown/" rel="prev" title="LaTeX in markdown">
                  <i class="fa fa-chevron-left"></i> LaTeX in markdown
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/04/02/latex/" rel="next" title="latex tutorial">
                  latex tutorial <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">thylakoids</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
